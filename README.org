#+TITLE: Claude Code Metrics Lab
#+DESCRIPTION: OpenTelemetry-based metrics tracking and analysis for Claude Code usage

[[https://github.com/astral-sh/uv][https://img.shields.io/badge/uv-0.5+-blue.svg]]
[[https://www.python.org/][https://img.shields.io/badge/python-3.10+-blue.svg]]
[[https://img.shields.io/badge/license-MIT-green.svg]]
[[https://img.shields.io/badge/status-draft-yellow.svg]]

* Overview

A pedagogical lab for learning telemetry implementation through practical Claude Code metrics tracking. This project emphasizes *understanding* over quick setup, providing workshops, exercises, and real-world examples.

* Features
- Real-time metrics collection via OpenTelemetry
- Custom Grafana dashboards with cost tracking
- Pedagogical debugging tools (OTLP interceptor, debug sink)
- Comprehensive test matrix for all configurations
- Cost analysis and model comparison
- Workshop materials for teaching telemetry concepts

* Quick Start

1. Copy =.env.sample= to =.env= and configure your environment
2. Start OTLP infrastructure (Prometheus, Grafana, OTLP Collector)
3. Use =make help= to see available commands
4. Try =make otlp-debug-sink= to capture raw telemetry data

* Real-World Metrics Example

The following dashboard screenshot shows actual Claude Code telemetry data captured during a 24-hour experiment:

[[file:docs/grafana-dashboard-example.png]]

*Key Observations:*
- *Total Cost*: $42.23 across 14 million tokens
- *Model Distribution*: Opus 4 dominated usage (11M tokens, $40.64)
- *Cost Variance*: 172x difference between Haiku ($0.24) and Opus 4 ($40.64)
- *Single Session*: All activity from one intensive session

This demonstrates the importance of telemetry for understanding AI operational costs. These metrics were collected via OpenTelemetry and visualized in Grafana.

* Learning Resources

** üìö Tutorials and Guides
- [[file:telemetry-approach-summary.org][Pedagogical Approach to Telemetry]] - Start here for the educational philosophy
- [[file:telemetry-experiment-phase1.org][Phase 1 Experiment Results]] - Real-world cost analysis and lessons learned
- [[file:technical-guide.org][Comprehensive Technical Guide]] - Deep dive into all components
- [[file:docs/telemetry-contracts.org][Telemetry Contracts & APIs]] - Detailed specifications for debugging

** üõ†Ô∏è Workshops and Exercises
- [[file:http-interceptor-workshop.org][HTTP Interceptor Workshop]] - Build telemetry interceptors from scratch
- [[file:netcat-otlp-exercises.org][Netcat OTLP Exercises]] - Learn OTLP protocol with simple tools
- [[file:generator.org][Dashboard Generator Tutorial]] - Create custom Grafana dashboards

** üìä Configuration and Examples
- [[file:dashboard.org][Dashboard Configuration]] - Pre-built dashboard examples
- [[file:.env.sample][Environment Configuration]] - Complete configuration reference
- [[file:test_matrix.py][Test Matrix]] - 54 test scenarios for telemetry configurations

* Available Tools

** Telemetry Debugging
#+begin_src bash
# Capture raw OTLP requests without forwarding
make otlp-debug-sink

# Intercept and forward OTLP requests
make otlp-interceptor

# Verbose interceptor with real-time analysis
make otlp-interceptor-verbose
#+end_src

** Metrics Simulation
#+begin_src bash
# Start metrics simulator (Brownian motion model)
make simulate

# Test with specific scenarios
make simulate-scenario SCENARIO=high_load

# Run simulator tests
make test-simulator
#+end_src

** Analysis and Dashboards
#+begin_src bash
# Run cost and usage analysis
make analyze

# Generate Grafana dashboards
make dashboards
make dashboards-dev
make dashboards-prod
#+end_src

** Development Tools
#+begin_src bash
# Code quality checks
make lint
make format

# Clean generated files
make clean

# Show all commands
make help
#+end_src

* Dashboard Architecture

#+begin_src mermaid :file docs/dashboard.png :tangle docs/dashboard.mmd
graph TB
    subgraph "Claude Code Metrics Enhanced Dashboard"
        subgraph "Row 1 - Key Metrics (4 units high)"
            S1[Total Sessions<br/>Counter: session_count_total]
            S2[Total Tokens Used<br/>Counter: token_usage_tokens_total]
            S3[Total Cost USD<br/>Counter: cost_usage_USD_total]
            S4[Total Commits<br/>Counter: commit_count_total]
        end
        
        subgraph "Row 2 - Usage Trends (8 units high)"
            TS1[Token Usage Rate by Type<br/>Time Series<br/>Grouped by: type]
            TS2[Cost Rate by Model<br/>Time Series<br/>Grouped by: model]
        end
        
        subgraph "Row 3 - Model Breakdown (8 units high)"
            T1[Usage by Model<br/>Table View<br/>Shows: Tokens & Cost per model]
        end
        
        subgraph "Row 4 - Activity Analysis (8 units high)"
            TS3[Hourly Token Usage<br/>Stacked Bar Chart<br/>Grouped by: model]
            TS4[Activity Rate<br/>Time Series<br/>Sessions/sec & Commits/sec]
        end
    end
    
    style S1 fill:#2d4a2b,stroke:#73bf69,color:#fff
    style S2 fill:#4a4a2b,stroke:#f2cc0c,color:#fff
    style S3 fill:#4a4a2b,stroke:#f2cc0c,color:#fff
    style S4 fill:#2d4a2b,stroke:#73bf69,color:#fff

#+end_src

#+RESULTS:
[[file:docs/dashboard.png]]

* Project Structure

#+begin_example
claude-code-metrics-lab/
‚îú‚îÄ‚îÄ .env.sample              # Environment configuration template
‚îú‚îÄ‚îÄ Makefile                 # All available commands (run 'make help')
‚îú‚îÄ‚îÄ README.org               # This file
‚îú‚îÄ‚îÄ CLAUDE.org              # Claude-specific configuration
‚îÇ
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ grafana-dashboard-example.png
‚îÇ   ‚îú‚îÄ‚îÄ telemetry-contracts.org
‚îÇ   ‚îî‚îÄ‚îÄ github-issue-*.md    # Issue templates
‚îÇ
‚îú‚îÄ‚îÄ src/                     # Analysis scripts
‚îÇ   ‚îú‚îÄ‚îÄ project_metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ cost_analyzer.py
‚îÇ   ‚îî‚îÄ‚îÄ session_analyzer.py
‚îÇ
‚îú‚îÄ‚îÄ scripts/                 # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ claude-metrics-simulator.py
‚îÇ   ‚îú‚îÄ‚îÄ generate_dashboards.py
‚îÇ   ‚îî‚îÄ‚îÄ otlp-http-interceptor.sh
‚îÇ
‚îú‚îÄ‚îÄ dashboards/              # Grafana dashboard JSON files
‚îú‚îÄ‚îÄ exports/                 # Analysis output directory
‚îî‚îÄ‚îÄ test_results/            # Test matrix results
#+end_example

* Requirements

- Python 3.8+ with uv package manager
- OpenTelemetry Collector (OTLP)
- Prometheus for metrics storage
- Grafana for visualization
- Claude API access with telemetry enabled
- netcat (nc) for debugging tools
- Optional: socat for advanced HTTP interception

* Contributing

This lab is designed for experimentation and learning. Contributions welcome:
1. Document new telemetry patterns
2. Add workshop exercises
3. Share dashboard improvements
4. Report cost anomalies

* References

- [[https://opentelemetry.io/docs/][OpenTelemetry Documentation]]
- [[https://prometheus.io/docs/][Prometheus Documentation]]
- [[https://grafana.com/docs/][Grafana Documentation]]
- GitHub Issues: [[file:docs/github-issue-telemetry-approach.md][Telemetry Approach]], [[file:docs/github-issue-prometheus-simulator.md][Prometheus Simulator]]